<HTML>
<HEAD>
<TITLE>Mobvoi  AI Lab</TITLE>
</HEAD>
<BODY>
<table>
<tr>
<td><img src="../2018-logo.jpg" width=150 height=70>
<td><h1><!--<a href="https://www.mobvoi.com/us/voice/ai-lab">-->Mobvoi AI Lab</h1>
</tr>
</table>

<!--
<p>Seattle is chosen as the location of Mobvoi AI Lab, 
  to be immersed in the forefront of AI technology.

<p>The lab is responsible for pushing Mobvoi technology forward. 
We work with several universities in both Asia and U.S. on innovation and
academic publications in the top international conferences, in speech and 
NLP technologies. Our scientists are also talented developers who deliver 
product-level codes  for our business and clients. 
Our technologies are driven by industry needs and standards, 
not just academic benchmarks. 

<p>Additionally we are actively expanding language coverage from Mandarin to others, such as Taiwan-accented Mandarin, Cantonese, and English. We have also started building German and Korean IoT services.
-->

<h2> Github contributions </h2>
Please visit <a href="https://github.com/mobvoi">https://github.com/mobvoi</a>
for the list of our github projects, including pybind11 branch for pykaldi.

<!--
<h2>Staff</h2>
<p> Dr. Mei-Yuh Hwang, 
a long-time speech veteran and an IEEE fellow. She was the director for the lab
between 6/2016-3/2020.

<p>
<img src="IEEE-Fellowship.png" width=400>

<p> A Ph.D. from Carnegie Mellon University, Mei-Yuh has built various speech recognition systems for many languages, both for DARPA researches (Resource Management, Wall Street Journal, ATIS, EARS, GALE) and for industry products (Microsoft Speech API, Cortana, and Skype speech-to-speech translation). She has gained extensive linguistic knowledge across many languages, via her experiences in Bing machine translation. She accumulated a deep understanding of Cortana NLP, in Mandarin, French, Italian, German and Spanish. She is a rare talent combining speech recognition, NLP, and linguistics in many languages.

<p> Dr. <a href="https://scholar.google.com/citations?user=b-zD42MAAAAJ&hl=en">Yangyang Shi</a>, a Ph.D. from Delft University, leads R&D in both speech recognition and natural language understanding. Dr. Shi worked at Microsoft China as an applied scientist from 2012-2016 on Cortana NLP, Microsoft Sunnyvale Bing search rank team in 2016-2017, and finally joined Mobvoi AI Lab in November 2017. Since joining Mobvoi, Dr. Shi has been very productive and has focused on speech recognition. He is often working on multiple projects simultaneously, on both production lines and research publications. The lab maintains 2-4 interns 
all year round who are usually under Dr. Shi's mentorship.

<p>Additional scientists are located in Beijing, Suzhou and Taipei. The teams work together while each site has its own focus:
<ul>
<li>Beijing: AI on-chip, hotword triggering and in-car personal assistents for VW.
<li>Suzhou: 2-3 dozens of automated customer-service support (CSS), by speech AI technologies
<li>Taipei: Far EasTone (FET) smart speakers and CSS business clients in Taiwan
<li>Seattle: English, and other languages.
</ul>
-->
<h2>Publications</h2>
<a class="anchor" id="pub"></a>
<!--
<li> 10 patents from the Seattle Lab itself, and more from Beijing and Suzhou in 2018.
<li> English system used on TicWatch 2 in 2017.
<li> Taiwan-accented Mandarin system used on <a href="https://www.youtube.com/watch?v=CXCHQFvQSmY">Far EasTone smart speakers</a>, 2018 and on.
-->
<ul>
<li> 2018 academic publications
<ol>
<li> <a href="2018/acl2018.pdf"> An end-to-end approach for handling unknown slot values in dialogue state tracking</a>, ACL 

<li> <a href="2018/domain-adversarial-training.pdf">Domain adversarial training for accented speech recognition</a>, ICASSP 

<li> <a href="2018/training-augmentation-adversarial.pdf"> Data augmentation with adversarial examples for robust speech recognition</a>, Interspeech

<li> <a href="2018/interspeech2018_probability_weighted_beamformer.pdf">A probability weighted beamformer for noise robust ASR</a> Interspeech 

<li> <a href="2018/source-critical.pdf">Source-critical reinforcement learning for transferring spoken language
understanding to a new language</a>, COLING 

<li> <a href="2018/iwaenc-2018.pdf"> A robust nonlinear microphone array postfilter for noise reduction</a>, IWAENC 

<li> <a href="2018/emnlp2018-b.pdf">A teacher-student framework for maintainable dialog manager</a>, EMNLP 

<li> <a href="2018/chime5.pdf"> Multiple Beamformers with Rover for the CHiME-5 Challenge</a>, Chime5 Workshop.
</ol>

<li> 2019 academic publications
<ol>
<li><a href="2019/icassp-yyshi-trust.pdf">Knowledge Distillation for Recurrent Neural Network Language Modeling  With Trust Regularization</a>, ICASSP
<li><a href="2019/icassp-yyshi-moe.pdf">End-to-end Speech Recognition Using High Rank LSTM-CTC Based Model</a>, ICASSP
<li><a href="2019/icassp-wang.pdf">Adversarial examples for improving end-to-end
attention-based small-footprint keyword spotting</a>, ICASSP
<li> 
<a href="2019/ACL-weikang.pdf">
Incremental learning from scratch for task-origented dialog systems</a>, ACL
<li><a href="2019/interspeech-slbu.pdf">
A novel method to correct steering vectors in MVDR beamformer for
noise robust ASR</a>, Interspeech
<li><a href="https://ieeexplore.ieee.org/document/8788563">Adversarial regularization for attention based end-to-end robust speech recognition</a>, IEEE Transactions on ASLP,
Vol 27, Issue 11, Nov. 2019, pp1826-1838.
<li><a href="2019/RPN.pdf">Region Proposal Network Based Small-Footprint Keyword Spotting</a>, IEEE Signal Processing Letters, Digital Object Identifier: 10.1109/LSP.2019.2936282
<li><!-- <a href="2019/APSIPA_2019.pdf">-->Multiple fixed beamformers with a spacial Wiener-form postfilter for far-field speech recognition</a>, APSIPA.
<li><!-- <a href="2019/emnlp-ijcnlp-2019.pdf">-->Are you for Real? Detecting Identity Fraud via Dialogue Interactions</a>, IJCNLP.
</ol>


<li> 2020 academic publications
<ol>
<li> <a href="2020/ICASSP2020_Max_pooling_KWS.pdf">Mining effective negative training samples for keyword spotting</a>, ICASSP 2020.
<!-- https://www.overleaf.com/9535441222vxddrmvnydvq 
<li> Regularize CTC with Maximum Entropy and another CTC For Speech Recognition, submitted to ICASSP 2020.
 https://www.overleaf.com/6169177741psjrqhbdvdtq -->
</ol>
</ul>

<h2>Reading materials</h2>
<ul>
<li><a href="https://www.isca-speech.org/iscaweb/index.php/archive/online-archive">ISCA Online Archive</a>
</ul>
<h4> Back to Mei-Yuh's <a href="../index.html"> professional homepage</a></h4>
</BODY>
</HTML>

